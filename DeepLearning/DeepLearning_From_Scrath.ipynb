{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[0.45514155 1.04806802]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1/1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    max_x = np.max(x)\n",
    "    x = x - max_x\n",
    "    exp_x = np.exp(x)\n",
    "    sum_exp = np.sum(exp_x)\n",
    "    return exp_x/sum_exp\n",
    "\n",
    "\n",
    "    \n",
    "def init_network():\n",
    "    network = {}\n",
    "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network['b1'] = np.array([0.1, 0.2, 0.3])\n",
    "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network['b2'] = np.array([0.1, 0.2])\n",
    "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network['b3'] = np.array([0.1, 0.2])\n",
    "    \n",
    "    return network\n",
    "\n",
    "def forward(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    z3 = a3\n",
    "    \n",
    "    return z3\n",
    "z = softmax(np.array([1010, 1000, 990]))\n",
    "print(np.sum(z))\n",
    "\n",
    "network = init_network()\n",
    "x = np.array([1.0, 0.5])\n",
    "y = forward(network, x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 10000 t:  10000\n",
      "[-0.06750315  0.0695926  -0.02730473  0.02256093 -0.22001474 -0.22038847\n",
      "  0.04862635  0.13499236  0.23342554 -0.0487357   0.10170191 -0.03076038\n",
      "  0.15482435  0.05212503  0.06017235 -0.03364862 -0.11218343 -0.26460695\n",
      " -0.03323386  0.13610415  0.06354368  0.04679805 -0.01621654 -0.05775835\n",
      " -0.03108677  0.10366164 -0.0845938   0.11665157  0.21852103  0.04437255\n",
      "  0.03378392 -0.01720384 -0.07383765  0.16152057 -0.10621249 -0.01646949\n",
      "  0.00913961  0.10238428  0.00916639 -0.0564299  -0.10607515  0.09892716\n",
      " -0.07136887 -0.06349134  0.12461706  0.02242282 -0.00047972  0.04527043\n",
      " -0.15179175  0.10716812]\n",
      "980\n",
      "10000\n",
      "accuacy rate is 0.098000\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "def img_show(img):\n",
    "    pil_img = Image.fromarray(np.uint8(img))\n",
    "    pil_img.show()\n",
    "    \n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    with open(\"./dataset/sample_weight.pkl\", \"rb\") as f:\n",
    "        network = pickle.load(f)\n",
    "    return network\n",
    "\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "    \n",
    "    return y\n",
    "\n",
    "x, t = get_data()\n",
    "network = init_network()\n",
    "print (\"x:\", len(x), \"t: \", len(t))\n",
    "accuracy_cnt = 0\n",
    "print(network['b1'])\n",
    "for i in range(len(x)):\n",
    "    y = predict(network, x[i])\n",
    "    p = np.argmax(y)\n",
    "    if p == t[i]:\n",
    "        accuracy_cnt += 1\n",
    "print(accuracy_cnt)\n",
    "print(len(x))\n",
    "print(\"accuacy rate is %f\" % (float(accuracy_cnt)/len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50)\n",
      "(50,)\n",
      "(50, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n",
      "x: 10000 t:  10000\n",
      "[-0.06750315  0.0695926  -0.02730473  0.02256093 -0.22001474 -0.22038847\n",
      "  0.04862635  0.13499236  0.23342554 -0.0487357   0.10170191 -0.03076038\n",
      "  0.15482435  0.05212503  0.06017235 -0.03364862 -0.11218343 -0.26460695\n",
      " -0.03323386  0.13610415  0.06354368  0.04679805 -0.01621654 -0.05775835\n",
      " -0.03108677  0.10366164 -0.0845938   0.11665157  0.21852103  0.04437255\n",
      "  0.03378392 -0.01720384 -0.07383765  0.16152057 -0.10621249 -0.01646949\n",
      "  0.00913961  0.10238428  0.00916639 -0.0564299  -0.10607515  0.09892716\n",
      " -0.07136887 -0.06349134  0.12461706  0.02242282 -0.00047972  0.04527043\n",
      " -0.15179175  0.10716812]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JH/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in subtract\n",
      "  \n",
      "/Users/JH/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in subtract\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1438\n",
      "Accuracy:0.1438\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "def get_data():\n",
    "    # 0〜255の値の範囲を0.0〜1.0に正規化させる\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    # 学習された重みパラメータをロード。\n",
    "    with open(\"dataset/sample_weight.pkl\", 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "    return network\n",
    "\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "\n",
    "    return y\n",
    "\n",
    "x, t = get_data()\n",
    "network = init_network()\n",
    "print(network['W1'].shape) # (784, 50) 1차원 배열로 reshape된 784(이미지의 크기 28x28)개의 입력, 첫번째 히든 레이어 50 \n",
    "print(network['b1'].shape) # 첫번째 히든 레이어에 대한 가중치\n",
    "print(network['W2'].shape) # (50, 100) 첫번째 히든 레이어 50개의 뉴런에서 두번째 히든 레이어 100개의 뉴런에 전달 \n",
    "print(network['b2'].shape) # ... 이하 생략 \n",
    "print(network['W3'].shape) # ...\n",
    "print(network['b3'].shape) # ...\n",
    "\n",
    "print (\"x:\", len(x), \"t: \", len(t))\n",
    "accuracy_cnt = 0\n",
    "print(network['b1'])\n",
    "for i in range(len(x)):\n",
    "    y = predict(network, x[i])\n",
    "    p = np.argmax(y)\n",
    "    if p == t[i]:\n",
    "        accuracy_cnt += 1\n",
    "print(accuracy_cnt)\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h)-f(x-h))/(2*h)\n",
    "\n",
    "def function(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        tmp = x[i]\n",
    "        x[i] = tmp + h\n",
    "        f1 = f(x)\n",
    "        \n",
    "        x[i] = tmp - h\n",
    "        f2 = f(x)\n",
    "        \n",
    "        grad[i] = (f1-f2)/(2*h)\n",
    "        x[i] = tmp\n",
    "        \n",
    "    return grad\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.1, step_num=100):\n",
    "    x = init_x\n",
    "    lis = []\n",
    "    for i in range(step_num):\n",
    "        lis.append(x.copy())\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr*grad\n",
    "        lis.append(x)\n",
    "    return np.array(lis)\n",
    "\n",
    "def function_2(x):\n",
    "    return np.sum(np.square(x))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_history = gradient_descent(function_2, np.array([3.0, 4.0]))\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JH/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-inf, -inf],\n",
       "       [-inf, -inf],\n",
       "       [-inf, -inf],\n",
       "       [-inf, -inf]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[np.arange(3), [0,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
